{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training: Gradient Boosting and Deep Learning\n",
    "\n",
    "This notebook demonstrates training and evaluation of:\n",
    "1. XGBoost\n",
    "2. LightGBM\n",
    "3. Transformer-based models\n",
    "\n",
    "on tabular datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../src')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from utils.data_loader import DataLoader\n",
    "from models import XGBoostClassifier, LightGBMClassifier, MLPClassifier, TransformerClassifier\n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose dataset\n",
    "dataset_name = 'breast_cancer'  # Options: 'breast_cancer', 'adult_income', 'bank_marketing'\n",
    "\n",
    "loader = DataLoader(dataset_name, random_state=42)\n",
    "X, y = loader.load_data()\n",
    "data = loader.prepare_data(X, y, test_size=0.2, scale_features=True)\n",
    "\n",
    "X_train = data['X_train']\n",
    "X_test = data['X_test']\n",
    "y_train = data['y_train']\n",
    "y_test = data['y_test']\n",
    "\n",
    "print(f\"Training set: {X_train.shape}\")\n",
    "print(f\"Test set: {X_test.shape}\")\n",
    "print(f\"Number of classes: {len(np.unique(y))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. XGBoost Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training XGBoost...\")\n",
    "xgb_model = XGBoostClassifier(n_estimators=100, max_depth=6, random_state=42)\n",
    "xgb_model.train(X_train, y_train)\n",
    "\n",
    "# Evaluate\n",
    "train_metrics = xgb_model.evaluate(X_train, y_train)\n",
    "test_metrics = xgb_model.evaluate(X_test, y_test)\n",
    "\n",
    "print(\"\\nXGBoost Training Metrics:\")\n",
    "for k, v in train_metrics.items():\n",
    "    print(f\"  {k}: {v:.4f}\" if v is not None else f\"  {k}: N/A\")\n",
    "\n",
    "print(\"\\nXGBoost Test Metrics:\")\n",
    "for k, v in test_metrics.items():\n",
    "    print(f\"  {k}: {v:.4f}\" if v is not None else f\"  {k}: N/A\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance\n",
    "xgb_importance = xgb_model.get_feature_importance(X_train.columns.tolist())\n",
    "print(\"\\nTop 10 Important Features (XGBoost):\")\n",
    "print(xgb_importance.head(10))\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.barh(xgb_importance['feature'][:20], xgb_importance['importance'][:20])\n",
    "plt.xlabel('Importance')\n",
    "plt.title('XGBoost Feature Importance')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. LightGBM Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training LightGBM...\")\n",
    "lgb_model = LightGBMClassifier(n_estimators=100, max_depth=6, random_state=42)\n",
    "lgb_model.train(X_train, y_train)\n",
    "\n",
    "# Evaluate\n",
    "train_metrics = lgb_model.evaluate(X_train, y_train)\n",
    "test_metrics = lgb_model.evaluate(X_test, y_test)\n",
    "\n",
    "print(\"\\nLightGBM Training Metrics:\")\n",
    "for k, v in train_metrics.items():\n",
    "    print(f\"  {k}: {v:.4f}\" if v is not None else f\"  {k}: N/A\")\n",
    "\n",
    "print(\"\\nLightGBM Test Metrics:\")\n",
    "for k, v in test_metrics.items():\n",
    "    print(f\"  {k}: {v:.4f}\" if v is not None else f\"  {k}: N/A\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance\n",
    "lgb_importance = lgb_model.get_feature_importance(X_train.columns.tolist())\n",
    "print(\"\\nTop 10 Important Features (LightGBM):\")\n",
    "print(lgb_importance.head(10))\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.barh(lgb_importance['feature'][:20], lgb_importance['importance'][:20])\n",
    "plt.xlabel('Importance')\n",
    "plt.title('LightGBM Feature Importance')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Transformer Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training Transformer...\")\n",
    "transformer_model = TransformerClassifier(\n",
    "    d_model=64,\n",
    "    nhead=4,\n",
    "    num_layers=2,\n",
    "    epochs=30,\n",
    "    batch_size=32,\n",
    "    learning_rate=0.001\n",
    ")\n",
    "transformer_model.train(X_train, y_train)\n",
    "\n",
    "# Evaluate\n",
    "train_metrics = transformer_model.evaluate(X_train, y_train)\n",
    "test_metrics = transformer_model.evaluate(X_test, y_test)\n",
    "\n",
    "print(\"\\nTransformer Training Metrics:\")\n",
    "for k, v in train_metrics.items():\n",
    "    print(f\"  {k}: {v:.4f}\" if v is not None else f\"  {k}: N/A\")\n",
    "\n",
    "print(\"\\nTransformer Test Metrics:\")\n",
    "for k, v in test_metrics.items():\n",
    "    print(f\"  {k}: {v:.4f}\" if v is not None else f\"  {k}: N/A\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare all models\n",
    "models = {\n",
    "    'XGBoost': xgb_model,\n",
    "    'LightGBM': lgb_model,\n",
    "    'Transformer': transformer_model\n",
    "}\n",
    "\n",
    "comparison_data = []\n",
    "for name, model in models.items():\n",
    "    metrics = model.evaluate(X_test, y_test)\n",
    "    metrics['Model'] = name\n",
    "    comparison_data.append(metrics)\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "comparison_df = comparison_df[['Model', 'accuracy', 'f1_score', 'precision', 'recall', 'roc_auc']]\n",
    "\n",
    "print(\"\\nModel Comparison on Test Set:\")\n",
    "print(comparison_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize comparison\n",
    "metrics_to_plot = ['accuracy', 'f1_score', 'precision', 'recall']\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for idx, metric in enumerate(metrics_to_plot):\n",
    "    ax = axes[idx]\n",
    "    data = comparison_df[['Model', metric]].dropna()\n",
    "    ax.bar(data['Model'], data[metric])\n",
    "    ax.set_title(metric.replace('_', ' ').title())\n",
    "    ax.set_ylabel('Score')\n",
    "    ax.set_ylim([0, 1])\n",
    "    ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated:\n",
    "- Training three different model types on tabular data\n",
    "- Evaluating model performance with multiple metrics\n",
    "- Comparing models across different metrics\n",
    "- Analyzing feature importance for tree-based models\n",
    "\n",
    "Next steps:\n",
    "- Apply SHAP and LIME explanations (see next notebook)\n",
    "- Calculate interpretability metrics\n",
    "- Compare explainability across models"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
