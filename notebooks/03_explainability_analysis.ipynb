{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explainability Analysis with SHAP and LIME\n",
    "\n",
    "This notebook demonstrates:\n",
    "1. SHAP explanations for models\n",
    "2. LIME explanations for models\n",
    "3. Interpretability metrics evaluation\n",
    "4. Comparison of explanation methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../src')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from utils.data_loader import DataLoader\n",
    "from models import XGBoostClassifier, LightGBMClassifier, TransformerClassifier\n",
    "from explainability import SHAPExplainer, LIMEExplainer\n",
    "from metrics import InterpretabilityMetrics\n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Data and Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "dataset_name = 'breast_cancer'\n",
    "loader = DataLoader(dataset_name, random_state=42)\n",
    "X, y = loader.load_data()\n",
    "data = loader.prepare_data(X, y, test_size=0.2, scale_features=True)\n",
    "\n",
    "X_train = data['X_train']\n",
    "X_test = data['X_test']\n",
    "y_train = data['y_train']\n",
    "y_test = data['y_test']\n",
    "\n",
    "print(f\"Training set: {X_train.shape}\")\n",
    "print(f\"Test set: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train XGBoost model\n",
    "print(\"Training XGBoost...\")\n",
    "model = XGBoostClassifier(n_estimators=100, max_depth=6, random_state=42)\n",
    "model.train(X_train, y_train)\n",
    "\n",
    "# Evaluate\n",
    "test_metrics = model.evaluate(X_test, y_test)\n",
    "print(\"\\nTest Metrics:\")\n",
    "for k, v in test_metrics.items():\n",
    "    print(f\"  {k}: {v:.4f}\" if v is not None else f\"  {k}: N/A\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SHAP Explanations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize SHAP explainer\n",
    "print(\"Generating SHAP explanations...\")\n",
    "shap_explainer = SHAPExplainer(model, X_train, model_type='tree')\n",
    "shap_values = shap_explainer.explain(X_test.head(100))\n",
    "\n",
    "print(f\"SHAP values shape: {np.array(shap_values).shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SHAP summary plot\n",
    "shap_explainer.plot_summary(X_test.head(100))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SHAP feature importance\n",
    "shap_importance = shap_explainer.get_feature_importance(X_test.head(100))\n",
    "print(\"\\nTop 10 Features by SHAP Importance:\")\n",
    "print(shap_importance.head(10))\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.barh(shap_importance['feature'][:20], shap_importance['importance'][:20])\n",
    "plt.xlabel('Mean |SHAP value|')\n",
    "plt.title('SHAP Feature Importance')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LIME Explanations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize LIME explainer\n",
    "print(\"Generating LIME explanations...\")\n",
    "lime_explainer = LIMEExplainer(model, X_train, X_train.columns.tolist())\n",
    "\n",
    "# Explain a single instance\n",
    "instance_idx = 0\n",
    "instance = X_test.iloc[instance_idx].values\n",
    "exp = lime_explainer.explain_instance(instance, num_features=10)\n",
    "\n",
    "print(f\"\\nLIME Explanation for instance {instance_idx}:\")\n",
    "print(f\"Prediction: {model.predict(X_test.iloc[[instance_idx]])[0]}\")\n",
    "print(f\"True label: {y_test.iloc[instance_idx]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot LIME explanation\n",
    "lime_explainer.plot_explanation(instance, num_features=10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LIME feature importance (aggregated)\n",
    "lime_importance = lime_explainer.get_feature_importance(X_test, num_samples=50)\n",
    "print(\"\\nTop 10 Features by LIME Importance:\")\n",
    "print(lime_importance.head(10))\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.barh(lime_importance['feature'][:20], lime_importance['importance'][:20])\n",
    "plt.xlabel('Mean Importance Score')\n",
    "plt.title('LIME Feature Importance (Aggregated)')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare SHAP and LIME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare feature importance rankings\n",
    "comparison = pd.merge(\n",
    "    shap_importance[['feature', 'importance']].rename(columns={'importance': 'SHAP'}),\n",
    "    lime_importance[['feature', 'importance']].rename(columns={'importance': 'LIME'}),\n",
    "    on='feature',\n",
    "    how='outer'\n",
    ").fillna(0)\n",
    "\n",
    "# Normalize for comparison\n",
    "comparison['SHAP_norm'] = comparison['SHAP'] / comparison['SHAP'].max()\n",
    "comparison['LIME_norm'] = comparison['LIME'] / comparison['LIME'].max()\n",
    "\n",
    "print(\"\\nTop 15 Features - SHAP vs LIME Comparison:\")\n",
    "print(comparison.head(15).to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter plot comparison\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.scatter(comparison['SHAP_norm'], comparison['LIME_norm'], alpha=0.6)\n",
    "plt.plot([0, 1], [0, 1], 'r--', label='Perfect Agreement')\n",
    "plt.xlabel('SHAP Importance (normalized)')\n",
    "plt.ylabel('LIME Importance (normalized)')\n",
    "plt.title('SHAP vs LIME Feature Importance')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpretability Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare explanations for metrics\n",
    "shap_explanations = []\n",
    "lime_explanations = []\n",
    "\n",
    "num_samples = min(50, len(X_test))\n",
    "for i in range(num_samples):\n",
    "    # SHAP\n",
    "    shap_exp = shap_explainer.explain_instance(X_test.iloc[i])\n",
    "    shap_explanations.append(shap_exp)\n",
    "    \n",
    "    # LIME\n",
    "    lime_exp = lime_explainer.explain_instance(X_test.iloc[i].values, num_features=10)\n",
    "    lime_exp_dict = dict(lime_exp.as_list())\n",
    "    feature_exp = {}\n",
    "    for feature in X_test.columns:\n",
    "        for key, val in lime_exp_dict.items():\n",
    "            if feature in key:\n",
    "                feature_exp[feature] = val\n",
    "                break\n",
    "        if feature not in feature_exp:\n",
    "            feature_exp[feature] = 0.0\n",
    "    lime_explanations.append(feature_exp)\n",
    "\n",
    "print(f\"Generated {len(shap_explanations)} SHAP and {len(lime_explanations)} LIME explanations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate interpretability metrics\n",
    "print(\"Calculating interpretability metrics...\\n\")\n",
    "\n",
    "# Feature importance stability\n",
    "importance_runs = [shap_importance, lime_importance]\n",
    "stability = InterpretabilityMetrics.feature_importance_stability(importance_runs, method='spearman')\n",
    "print(f\"Feature Importance Stability (Spearman): {stability:.4f}\")\n",
    "\n",
    "# Explanation consistency\n",
    "consistency = InterpretabilityMetrics.explanation_consistency(\n",
    "    shap_explanations, lime_explanations, metric='cosine'\n",
    ")\n",
    "print(f\"SHAP-LIME Explanation Consistency: {consistency:.4f}\")\n",
    "\n",
    "# Feature agreement\n",
    "agreement_5 = InterpretabilityMetrics.feature_agreement(shap_importance, lime_importance, top_k=5)\n",
    "agreement_10 = InterpretabilityMetrics.feature_agreement(shap_importance, lime_importance, top_k=10)\n",
    "print(f\"Feature Agreement (top-5): {agreement_5:.4f}\")\n",
    "print(f\"Feature Agreement (top-10): {agreement_10:.4f}\")\n",
    "\n",
    "# Explanation fidelity\n",
    "shap_fidelity = InterpretabilityMetrics.explanation_fidelity(\n",
    "    model, X_test.head(num_samples), shap_explanations, top_k=5\n",
    ")\n",
    "lime_fidelity = InterpretabilityMetrics.explanation_fidelity(\n",
    "    model, X_test.head(num_samples), lime_explanations, top_k=5\n",
    ")\n",
    "print(f\"SHAP Explanation Fidelity: {shap_fidelity:.4f}\")\n",
    "print(f\"LIME Explanation Fidelity: {lime_fidelity:.4f}\")\n",
    "\n",
    "# Explanation complexity\n",
    "shap_complexity = InterpretabilityMetrics.explanation_complexity(shap_explanations)\n",
    "lime_complexity = InterpretabilityMetrics.explanation_complexity(lime_explanations)\n",
    "print(f\"SHAP Explanation Complexity: {shap_complexity:.2f} features\")\n",
    "print(f\"LIME Explanation Complexity: {lime_complexity:.2f} features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize metrics\n",
    "metrics_data = {\n",
    "    'Metric': ['Stability', 'Consistency', 'Agreement\\n(top-5)', 'Agreement\\n(top-10)'],\n",
    "    'Score': [stability, consistency, agreement_5, agreement_10]\n",
    "}\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Interpretability metrics\n",
    "ax1.bar(metrics_data['Metric'], metrics_data['Score'])\n",
    "ax1.set_ylabel('Score')\n",
    "ax1.set_title('Interpretability Metrics')\n",
    "ax1.set_ylim([0, 1])\n",
    "ax1.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Fidelity comparison\n",
    "fidelity_data = ['SHAP', 'LIME']\n",
    "fidelity_scores = [shap_fidelity, lime_fidelity]\n",
    "ax2.bar(fidelity_data, fidelity_scores)\n",
    "ax2.set_ylabel('Fidelity Score')\n",
    "ax2.set_title('Explanation Fidelity')\n",
    "ax2.grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated:\n",
    "- Generation of SHAP explanations for tree-based models\n",
    "- Generation of LIME explanations for any model\n",
    "- Comparison of SHAP and LIME feature importance\n",
    "- Calculation of rigorous interpretability metrics:\n",
    "  - Feature importance stability\n",
    "  - Explanation consistency\n",
    "  - Feature agreement\n",
    "  - Explanation fidelity\n",
    "  - Explanation complexity\n",
    "\n",
    "These metrics provide quantitative evaluation of model interpretability beyond just accuracy."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
