d_model: 128
nhead: 4
num_layers: 2
dim_feedforward: 256
dropout: 0.4
weight_decay: 5e-4

mixup:
  enabled: false
  alpha: 0.0

gaussian_noise:
  enabled: true
  std: 0.01  # σ=0.01-0.02 for tiny Gaussian noise on numerics during training

label_smoothing: 0.02

optimizer:
  name: AdamW
  lr: 1e-3
  betas: [0.9, 0.999]
  eps: 1e-8

swa:
  enabled: true
  final_epochs: 30

scheduler:
  name: cosine
  warmup_proportion: 0.08
  min_lr: 0.0

training:
  batch_size: 32
  epochs: 300
  early_stopping:
    enabled: true
    monitor: pr_auc
    patience: 50
  auto_device: true

random_seed: 42

notes: |
  Simplified Transformer for breast cancer (tiny dataset, n=569): d_model=128, heads=4, layers=2,
  FFN=256, dropout=0.4, weight decay=5e-4. Keep it tiny; MLP > Transformer on this dataset unless
  you add synthetic features. AdamW lr=1e-3, cosine scheduler, 300 epochs with early stopping
  (patience=50). Label smoothing=0.02, Gaussian noise (σ=0.01) for augmentation. SWA last 30 epochs.
  MixUp disabled (not needed for this tiny dataset). Expect parity with MLP, not dominance.
