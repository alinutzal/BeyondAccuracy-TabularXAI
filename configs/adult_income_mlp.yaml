hidden_dims: [512, 512, 256, 256]
activation: geglu
layer_norm_per_block: true
dropout: 0.25
embedding_dropout: 0.1
weight_decay: 2e-4

mixup:
  enabled: true
  alpha: 0.2

label_smoothing: 0.03

optimizer:
  name: AdamW
  lr: 1e-3
  betas: [0.9, 0.999]
  eps: 1e-8

scheduler:
  name: cosine
  warmup_proportion: 0.08  # fraction of total training steps used for linear warmup
  min_lr: 0.0

training:
  batch_size: 32
  epochs: 200
  early_stopping:
    enabled: true
    monitor: oof_auc
    patience: 10
  auto_device: true  # if true, code should select 'cuda' when available else 'cpu'

random_seed: 42

evaluation:
  type: 5fold_stratified_cv # hash_timeish_split

notes: |
  Model uses GEGLU activations with LayerNorm in each block. Dropout=0.25,
  embedding dropout=0.1, weight decay=2e-4. MixUp alpha=0.2 and label smoothing=0.03.
  Optimizer: AdamW lr=1e-3 with cosine decay scheduler and 8% warmup. 200 epochs
  with early stopping on out-of-fold (OOF) AUC.
