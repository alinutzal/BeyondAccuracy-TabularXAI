feature_tokenizer:
  numeric:
    scaler: learnable_linear
    quantile_embedding:
      enabled: false
  categorical:
    embedding_dim: 32
  column_id_embedding_dim: 32
  type_embedding_dim: 16

transformer:
  d_model: 256
  nhead: 4
  num_layers: 6
  dim_feedforward: 640
  ffn_activation: geglu
  dropout: 0.35
  stochastic_depth: 0.15

gating_tower:
  enabled: true
  hidden_dim: 256
  residual: true

loss:
  type: focal  # options: 'focal' or 'weighted_bce'
  focal_gamma: 2.0

optimizer:
  name: AdamW
  lr: 8e-4
  betas: [0.9, 0.999]
  eps: 1e-8
  weight_decay: 3e-4

scheduler:
  name: cosine
  warmup_proportion: 0.08
  min_lr: 0.0

training:
  batch_size: 1024
  epochs: 300
  early_stopping:
    enabled: true
    monitor: pr_auc
    patience: 30
  auto_device: true

swa:
  enabled: true
  final_epochs: 20

monitoring:
  metrics: [pr_auc, brier_score]

post_train:
  isotonic_calibration: true
  calibration_holdout_fraction: 0.1

random_seed: 42

notes: |
  Transformer: d_model=256, heads=4, depth=6, FFN=640, GEGLU, dropout=0.35, stochastic
  depth=0.15. A small gating tower (two-layer MLP 256->1 with residual) sits on top for
  calibration. Loss supports focal (gamma=2) or weighted BCE. Monitor PR-AUC and Brier.
  Post-train isotonic calibration can be run on a held-out fold.
